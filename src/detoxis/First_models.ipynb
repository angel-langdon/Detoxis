{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "First_models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBMggrL605f1"
      },
      "source": [
        "# Sesion 2 del proyecto de LNR\n",
        "\n",
        "## Estudiantes\n",
        "- Ignacio Cano Navarro\n",
        "- Angel Langdon Villamayor\n",
        "\n",
        "## Primeros modelos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4UGQEPW1Q7H"
      },
      "source": [
        "### Preproceso\n",
        "\n",
        "Importaremos las librerías y el preprocesado realizado en la entrega anterior"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-k4x-Q8Q5Xf",
        "outputId": "8b68f137-900d-4463-c0f3-36c4bca9f17e"
      },
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "\n",
        "# necessary packages\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "I6E9E6E_RLVG",
        "outputId": "6c5ff458-2957-4030-f661-c7f606052a19"
      },
      "source": [
        "# Preprocessing\n",
        "def delete_stop_words(comment):\n",
        "    spanish_stopwords = stopwords.words(\"spanish\")\n",
        "    return \" \".join([w for w in comment.split() if w not in spanish_stopwords])\n",
        "\n",
        "def steam(text, stemmer):\n",
        "    stemmed_text = [stemmer.stem(word) for word in word_tokenize(text)]\n",
        "    return \" \".join(stemmed_text)\n",
        "\n",
        "def clean_text_column(df, col, stemmer):\n",
        "    \"\"\"Normalizes a string column to have a processed format \n",
        "    Arguments:\n",
        "      df (pd.DataFrame): the dataframe that contains the column to normalize\n",
        "      col (str): the dataframe column to normalize\n",
        "      steammer (nltk.steam.SnowballStammer): the steammer to use for \n",
        "          steamming the text\n",
        "    Returns:\n",
        "      The dataframe with the preprocessed column\n",
        "    \"\"\"\n",
        "    df = df.copy() # copy the dataframe avoid modifying the original\n",
        "    # Make the comments to lowercase \n",
        "    df[col] = df[col].str.lower()\n",
        "    # Delete the stop words\n",
        "    df[col] = [delete_stop_words(c) for c in df[col]]\n",
        "    # Replace underscores and hyphens with spaces \n",
        "    df[col] = df[col].str.replace(\"_\", \" \")\n",
        "    df[col] = df[col].str.replace(\"-\", \" \")\n",
        "    # Create the regex to delete the urls, usernames and emojis\n",
        "    urls = r'https?://[\\S]+'\n",
        "    users = r'@[\\S]+'\n",
        "    emojis = r'[\\U00010000-\\U0010ffff]'\n",
        "    hashtags = r'\\s#[\\S]+'\n",
        "    # Join the regex\n",
        "    expr = f'''({\"|\".join([urls,\n",
        "                           users,\n",
        "                           hashtags,\n",
        "                           emojis])})'''\n",
        "    # Replace the urls, users and emojis with empty string\n",
        "    df[col] = df[col].str.replace(expr, \"\", regex=True)                      \n",
        "    # Get only the words of the text\n",
        "    df[col] = df[col].str.findall(\"\\w+\").str.join(\" \")\n",
        "    # Delete the numbers\n",
        "    df[col] = df[col].str.replace(\"[0-9]+\", \"\",regex=True)\n",
        "    # Steam the words of the text for each text in the specified column\n",
        "    #df[col] = [steam(c, stemmer) for c in  df[col]]\n",
        "    return df\n",
        "\n",
        "\n",
        "# Initialize the steammer to Spanish language\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "# read the data\n",
        "df_original = pd.read_csv(\"train.csv\") \n",
        "# Normalize the \"comment\" column\n",
        "df = clean_text_column(df_original, \"comment\", stemmer)\n",
        "df.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>thread_id</th>\n",
              "      <th>comment_id</th>\n",
              "      <th>reply_to</th>\n",
              "      <th>comment_level</th>\n",
              "      <th>comment</th>\n",
              "      <th>argumentation</th>\n",
              "      <th>constructiveness</th>\n",
              "      <th>positive_stance</th>\n",
              "      <th>negative_stance</th>\n",
              "      <th>target_person</th>\n",
              "      <th>target_group</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>mockery</th>\n",
              "      <th>insult</th>\n",
              "      <th>improper_language</th>\n",
              "      <th>aggressiveness</th>\n",
              "      <th>intolerance</th>\n",
              "      <th>toxicity</th>\n",
              "      <th>toxicity_level</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CR</td>\n",
              "      <td>0_000</td>\n",
              "      <td>0_002</td>\n",
              "      <td>0_002</td>\n",
              "      <td>1</td>\n",
              "      <td>pensó zumo restar</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CR</td>\n",
              "      <td>0_001</td>\n",
              "      <td>0_003</td>\n",
              "      <td>0_003</td>\n",
              "      <td>1</td>\n",
              "      <td>gusta afeitado seco gente</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CR</td>\n",
              "      <td>0_002</td>\n",
              "      <td>0_004</td>\n",
              "      <td>0_004</td>\n",
              "      <td>1</td>\n",
              "      <td>asi gusta maten alta mar mas inmigrantes asi p...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CR</td>\n",
              "      <td>0_003</td>\n",
              "      <td>0_005</td>\n",
              "      <td>0_005</td>\n",
              "      <td>1</td>\n",
              "      <td>loss mas valientes mejor cortan cabezas vosotr...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CR</td>\n",
              "      <td>0_004</td>\n",
              "      <td>0_006</td>\n",
              "      <td>0_006</td>\n",
              "      <td>1</td>\n",
              "      <td>costumbres</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  topic thread_id comment_id  ... intolerance  toxicity toxicity_level\n",
              "0    CR     0_000      0_002  ...           0         1              1\n",
              "1    CR     0_001      0_003  ...           0         1              1\n",
              "2    CR     0_002      0_004  ...           1         1              2\n",
              "3    CR     0_003      0_005  ...           0         1              1\n",
              "4    CR     0_004      0_006  ...           0         1              1\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "YKtTbJ6LROwg",
        "outputId": "926c1fa0-6621-4265-fb60-b1f35a82b1cc"
      },
      "source": [
        "# Create a TF-IDF with ngrams \n",
        "tfidf = TfidfVectorizer(ngram_range=(1,1))\n",
        "# Fit with the comments \n",
        "features = tfidf.fit_transform(df[\"comment\"])\n",
        "# Get the feature extraction matrix\n",
        "df_features = pd.DataFrame(features.todense(),\n",
        "             columns= tfidf.get_feature_names())\n",
        "# Print the first comment\n",
        "print(df_original[\"comment\"].iloc[0])\n",
        "# Print the sorted by probability first row of the matrix\n",
        "df_features.sort_values(by=0, axis=1, ascending=False).head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pensó: Zumo para restar.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pensó</th>\n",
              "      <th>restar</th>\n",
              "      <th>zumo</th>\n",
              "      <th>opino</th>\n",
              "      <th>operativas</th>\n",
              "      <th>opinadores</th>\n",
              "      <th>opinamos</th>\n",
              "      <th>opinan</th>\n",
              "      <th>opinando</th>\n",
              "      <th>opinar</th>\n",
              "      <th>opine</th>\n",
              "      <th>opines</th>\n",
              "      <th>opinion</th>\n",
              "      <th>opiniones</th>\n",
              "      <th>opinión</th>\n",
              "      <th>opniones</th>\n",
              "      <th>operación</th>\n",
              "      <th>oponen</th>\n",
              "      <th>oportunas</th>\n",
              "      <th>oportunidad</th>\n",
              "      <th>oportunidades</th>\n",
              "      <th>oposicion</th>\n",
              "      <th>oposición</th>\n",
              "      <th>opresor</th>\n",
              "      <th>opresores</th>\n",
              "      <th>oprimidas</th>\n",
              "      <th>oprimido</th>\n",
              "      <th>oprimidos</th>\n",
              "      <th>operandi</th>\n",
              "      <th>operaciones</th>\n",
              "      <th>optas</th>\n",
              "      <th>onegetas</th>\n",
              "      <th>olía</th>\n",
              "      <th>olímpicamente</th>\n",
              "      <th>omiso</th>\n",
              "      <th>omite</th>\n",
              "      <th>omiten</th>\n",
              "      <th>omites</th>\n",
              "      <th>omitir</th>\n",
              "      <th>omnipresente</th>\n",
              "      <th>...</th>\n",
              "      <th>elijo</th>\n",
              "      <th>emigraban</th>\n",
              "      <th>emigrado</th>\n",
              "      <th>emigramos</th>\n",
              "      <th>emigran</th>\n",
              "      <th>emigrante</th>\n",
              "      <th>emigrantes</th>\n",
              "      <th>emigrar</th>\n",
              "      <th>emigraran</th>\n",
              "      <th>emigraron</th>\n",
              "      <th>emigré</th>\n",
              "      <th>emigró</th>\n",
              "      <th>embarcación</th>\n",
              "      <th>embarcaciones</th>\n",
              "      <th>embajadss</th>\n",
              "      <th>embajadas</th>\n",
              "      <th>elimina</th>\n",
              "      <th>eliminación</th>\n",
              "      <th>eliminar</th>\n",
              "      <th>eliminas</th>\n",
              "      <th>elimine</th>\n",
              "      <th>elisa</th>\n",
              "      <th>elite</th>\n",
              "      <th>eljueves</th>\n",
              "      <th>ella</th>\n",
              "      <th>ellas</th>\n",
              "      <th>elle</th>\n",
              "      <th>ello</th>\n",
              "      <th>ellos</th>\n",
              "      <th>elmundotoday</th>\n",
              "      <th>elpais</th>\n",
              "      <th>elplural</th>\n",
              "      <th>elsaltodiario</th>\n",
              "      <th>elíptica</th>\n",
              "      <th>em</th>\n",
              "      <th>ema</th>\n",
              "      <th>email</th>\n",
              "      <th>emanan</th>\n",
              "      <th>emanuel</th>\n",
              "      <th>útiles</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.630681</td>\n",
              "      <td>0.608146</td>\n",
              "      <td>0.482079</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 14289 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      pensó    restar      zumo  opino  ...  email  emanan  emanuel  útiles\n",
              "0  0.630681  0.608146  0.482079    0.0  ...    0.0     0.0      0.0     0.0\n",
              "\n",
              "[1 rows x 14289 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1as1PvjTC7q"
      },
      "source": [
        "# Primer modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQb5zIvRnqBt"
      },
      "source": [
        "## Consideraciones\n",
        "\n",
        "Se va a utilizar gridsearchCV de sklearn para encontrar los parámetros óptimos de cada uno de los modelos a utilizar. \n",
        "\n",
        "También se va a utilizar el parámetro random_state para que los resultados sean los mismos siempre.\n",
        "\n",
        "Aunque quede fuera del alcanze de este informe se va a intentar una sencilla implementación en Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttb19_zDl4Jb"
      },
      "source": [
        "## Variable Toxicity\n",
        "\n",
        "Vamos a empezar con la predicción de la variable toxicity. (binaria) Posteriormente pasaremos a predecir la variable toxicity_level, la cual suponemos que será mucho más complicada para los siguientes modelos que implementaremos de predecir. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7rS57D_3mSd"
      },
      "source": [
        "## SVM\n",
        "\n",
        "Estos métodos están propiamente relacionados con problemas de clasificación. Dado un conjunto de ejemplos de entrenamiento (de muestras) podemos etiquetar las clases y entrenar una SVM para construir un modelo que prediga la clase de una nueva muestra. Intuitivamente, una SVM es un modelo que representa a los puntos de muestra en el espacio, separando las clases a 2 espacios lo más amplios posibles mediante un hiperplano de separación definido como el vector entre los 2 puntos, de las 2 clases, más cercanos al que se llama vector soporte. Cuando las nuevas muestras se ponen en correspondencia con dicho modelo, en función de los espacios a los que pertenezcan, pueden ser clasificadas a una o la otra clase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQCP8Bui3zeO"
      },
      "source": [
        "Vamos a empezar pues cargando las librerías necesarias para implementar los diferentes modelos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2iKixzLTCHF"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "from sklearn import svm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "f1 = make_scorer(f1_score , average='macro')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_hUILrg39pg"
      },
      "source": [
        "Se ha utilizado la función GridSearchCV de sklearn con el objetivo de probar diferentes parámetros y encontrar el óptimo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gp-AtOw-h36K",
        "outputId": "681dd27b-0374-4ae7-9e3f-2a6e9e5ccfdc"
      },
      "source": [
        "parameters = {'kernel':['poly', 'rbf', 'sigmoid'], 'degree':[1, 3, 5], 'C':[0.5, 1, 1.5]}\n",
        "\n",
        "X,y = df_features, df_original['toxicity']\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.2, random_state=20)\n",
        "\n",
        "clf = GridSearchCV(svm.SVC(class_weight='balanced', random_state=20), parameters, scoring = f1)\n",
        "clf.fit(X_train, y_train)\n",
        "pred = clf.predict(X_eval)\n",
        "score = f1_score(y_eval, pred, average='macro')\n",
        "print(score)\n",
        "print(clf.get_params())\n",
        "clf.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.59490103021183\n",
            "{'cv': None, 'error_score': nan, 'estimator__C': 1.0, 'estimator__break_ties': False, 'estimator__cache_size': 200, 'estimator__class_weight': 'balanced', 'estimator__coef0': 0.0, 'estimator__decision_function_shape': 'ovr', 'estimator__degree': 3, 'estimator__gamma': 'scale', 'estimator__kernel': 'rbf', 'estimator__max_iter': -1, 'estimator__probability': False, 'estimator__random_state': 20, 'estimator__shrinking': True, 'estimator__tol': 0.001, 'estimator__verbose': False, 'estimator': SVC(C=1.0, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
            "    max_iter=-1, probability=False, random_state=20, shrinking=True, tol=0.001,\n",
            "    verbose=False), 'iid': 'deprecated', 'n_jobs': None, 'param_grid': {'degree': [3]}, 'pre_dispatch': '2*n_jobs', 'refit': True, 'return_train_score': False, 'scoring': make_scorer(f1_score, average=macro), 'verbose': 0}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'degree': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0olQtWgxcudo"
      },
      "source": [
        "## Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu9I3CnA4006"
      },
      "source": [
        "Un árbol de decisión es un modelo predictivo que divide el espacio de los predictores agrupando observaciones con valores similares para la variable respuesta o dependiente.\n",
        "\n",
        "Para dividir el espacio muestral en sub-regiones es preciso aplicar una serie de reglas o decisiones, para que cada sub-región contenga la mayor proporción posible de individuos de una de las poblaciones.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09c5kzBQcrsR"
      },
      "source": [
        "tree_para = {'criterion':['gini', 'entropy']}\n",
        "\n",
        "clf = GridSearchCV(DecisionTreeClassifier(class_weight='balanced', random_state=20), tree_para, scoring=f1)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "pred = clf.predict(X_eval)\n",
        "score = f1_score(y_eval, pred, average='macro')\n",
        "\n",
        "print(score)\n",
        "print(clf.get_params())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDyw_4qkdkkT"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9VE9VG46AAG"
      },
      "source": [
        "Los árboles de decisión tienen la tendencia de sobre-ajustar (overfit). Esto quiere decir que tienden a aprender muy bien los datos de entrenamiento pero su generalización no es tan buena. Una forma de mejorar la generalización de los árboles de decisión es usar regularización. Para mejorar mucho más la capacidad de generalización de los árboles de decisión, deberemos combinar varios árboles\n",
        "\n",
        "Un Random Forest es un conjunto (ensemble) de árboles de decisión combinados con bagging. Al usar bagging, lo que en realidad está pasando, es que distintos árboles ven distintas porciones de los datos. Ningún árbol ve todos los datos de entrenamiento. Esto hace que cada árbol se entrene con distintas muestras de datos para un mismo problema. De esta forma, al combinar sus resultados, unos errores se compensan con otros y tenemos una predicción que generaliza mejor.\n",
        "\n",
        "Es por eso que vamos a probar el método de Random Forest a ver si este mejora el resultado de Decision Trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aqzFVQRQdqja",
        "outputId": "7c4e6759-f987-4d43-cb0f-c77c9ff8f07e"
      },
      "source": [
        "rf = RandomForestClassifier(n_jobs=-1, oob_score = True, class_weight='balanced', random_state=20) \n",
        "\n",
        "param_grid = { \n",
        "    'n_estimators': [100, 150, 200],\n",
        "    'max_features': ['auto', 'log2']\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(estimator=rf, param_grid=param_grid, scoring=f1)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "pred = clf.predict(X_eval)\n",
        "score = f1_score(y_eval, pred, average='macro')\n",
        "print(score)\n",
        "print(print(clf.get_params()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6144942900663756\n",
            "{'cv': None, 'error_score': nan, 'estimator__bootstrap': True, 'estimator__ccp_alpha': 0.0, 'estimator__class_weight': 'balanced', 'estimator__criterion': 'gini', 'estimator__max_depth': None, 'estimator__max_features': 'auto', 'estimator__max_leaf_nodes': None, 'estimator__max_samples': None, 'estimator__min_impurity_decrease': 0.0, 'estimator__min_impurity_split': None, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__min_weight_fraction_leaf': 0.0, 'estimator__n_estimators': 100, 'estimator__n_jobs': -1, 'estimator__oob_score': True, 'estimator__random_state': 20, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
            "                       criterion='gini', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                       n_jobs=-1, oob_score=True, random_state=20, verbose=0,\n",
            "                       warm_start=False), 'iid': 'deprecated', 'n_jobs': None, 'param_grid': {'n_estimators': [100, 150, 200], 'max_features': ['auto', 'log2']}, 'pre_dispatch': '2*n_jobs', 'refit': True, 'return_train_score': False, 'scoring': make_scorer(f1_score, average=macro), 'verbose': 0}\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Md03vFvXUog"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29iHFCsiXf70"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "La Regresión Logística, es un método de regresión que permite estimar la probabilidad de una variable cualitativa binaria en función de una o más variables cuantitativas. Una de las principales aplicaciones de la regresión logística es la de clasificación binaria, en el que las observaciones se clasifican en un grupo u otro dependiendo del valor que tome la variable empleada como predictor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3cR6X04GZanN",
        "outputId": "493ec74c-0b96-43ba-ca8b-08d780913d91"
      },
      "source": [
        "param_grid = { \n",
        "    'C': [0.5, 1, 2],\n",
        "    'solver':['liblinear', 'lbfgs', 'newton-cg']\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(LogisticRegression(class_weight='balanced', random_state=20), param_grid, scoring = f1)\n",
        "clf.fit(X_train, y_train)\n",
        "pred = clf.predict(X_eval)\n",
        "score = f1_score(y_eval, pred, average='macro')\n",
        "print(score)\n",
        "print(clf.get_params())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6503144499280026\n",
            "{'cv': None, 'error_score': nan, 'estimator__C': 1.0, 'estimator__class_weight': 'balanced', 'estimator__dual': False, 'estimator__fit_intercept': True, 'estimator__intercept_scaling': 1, 'estimator__l1_ratio': None, 'estimator__max_iter': 100, 'estimator__multi_class': 'auto', 'estimator__n_jobs': None, 'estimator__penalty': 'l2', 'estimator__random_state': 20, 'estimator__solver': 'lbfgs', 'estimator__tol': 0.0001, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=20, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False), 'iid': 'deprecated', 'n_jobs': None, 'param_grid': {'C': [0.5, 1, 2], 'solver': ['liblinear', 'lbfgs', 'newton-cg']}, 'pre_dispatch': '2*n_jobs', 'refit': True, 'return_train_score': False, 'scoring': make_scorer(f1_score, average=macro), 'verbose': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb0w2WbFlekv"
      },
      "source": [
        "Como podemos ver, en general los modelos utilizados no son capaces de superar el 0.66 de F1-score en la predicción de la variable toxicity. Esto puede ser debido a la variabilidad del dataset, unido a que son modelos sencillos y que con los pocos datos que disponemos, no son capaces de generalizar lo suficiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRuItWbumRpn"
      },
      "source": [
        "## Variable Toxicity_Level\n",
        "\n",
        "Ahora procederemos a aplicar los mismos modelos a la variable Toxicity_level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnp7ZEMrmbGN"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UVbuij1pmeMp",
        "outputId": "2d84064d-2044-4996-a1c5-c6b51a18f058"
      },
      "source": [
        "parameters = {'degree':[1,3,5],\n",
        "              'kernel': ['poly', 'rbf'],\n",
        "              'C':[0.5,1,1.5]}\n",
        "\n",
        "X,y = df_features, df_original['toxicity_level']\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.2, random_state=20)\n",
        "\n",
        "clf = GridSearchCV(svm.SVC(class_weight='balanced', random_state=20), parameters, scoring = f1)\n",
        "clf.fit(X_train, y_train)\n",
        "pred = clf.predict(X_eval)\n",
        "score = f1_score(y_eval, pred, average='macro')\n",
        "print(score)\n",
        "print(clf.get_params())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c2bbea6c1d6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdLGA8G_nCaC"
      },
      "source": [
        "## Decision Trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KZhr8_jAnFqE"
      },
      "source": [
        "tree_para = {'criterion':['gini', 'entropy']}\n",
        "\n",
        "clf = GridSearchCV(DecisionTreeClassifier(DecisionTreeClassifier(class_weight='balanced', random_state=20)), tree_para, scoring=f1)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "pred = clf.predict(X_eval)\n",
        "score = f1_score(y_eval, pred, average='macro')\n",
        "\n",
        "print(score)\n",
        "print(clf.get_params())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdGZ6QUHnKtt"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JhnWIotPnRKJ"
      },
      "source": [
        "rf = RandomForestClassifier(n_jobs=-1, oob_score = True, class_weight='balanced', random_state=20) \n",
        "\n",
        "param_grid = { \n",
        "    'n_estimators': [100, 150, 200],\n",
        "    'max_features': ['auto', 'log2']\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(estimator=rf, param_grid=param_grid, scoring=f1)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "pred = clf.predict(X_eval)\n",
        "score = f1_score(y_eval, pred, average='macro')\n",
        "print(score)\n",
        "print(print(clf.get_params()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oIYkUBQnNh6"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2gA4tgIlnQiU"
      },
      "source": [
        "param_grid = { \n",
        "    'solver':['liblinear', 'lbfgs', 'newton-cg']\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(LogisticRegression(class_weight='balanced', random_state=20), param_grid, scoring = f1)\n",
        "clf.fit(X_train, y_train)\n",
        "pred = clf.predict(X_eval)\n",
        "score = f1_score(y_eval, pred, average='macro')\n",
        "print(score)\n",
        "print(clf.get_params())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cHPxz0GnUnb"
      },
      "source": [
        "Como hemos previsto, el F1-score ha sido mucho menor al intentar predecir la variable toxicity_level, de hecho, el mejor modelo en este caso ha sido logistic regression con un 0.35 de F1-score. Para posteriores informes seria interesante aplicar CV para unos resultados mas exactos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6jDQT9TfbJo"
      },
      "source": [
        "# BONUS\n",
        "\n",
        "Implementamos un modelo secuencial de keras con una capa de convolución.\n",
        "Como no es el objetivo de este informe, sino que se hace como algo \"extra\" no se explicará con tanto detalle ya que simplemente queríamos comprobar como funcionaba esta librería para el procesamiento de textos y el F1-score que devolvía dicho modelo. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bXQEe-X6Nvp"
      },
      "source": [
        "## Preprocesamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-sakFiBHfibD"
      },
      "source": [
        "text_data = list(df['comment'])\n",
        "leng = [len(txt) for txt in text_data]\n",
        "X,y = df['comment'], df['toxicity']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=20)\n",
        "X_train, X_test, y_train, y_test = list(X_train), list(X_test), np.array(y_train), np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Aw62hEK6QwQ"
      },
      "source": [
        "\n",
        "## Tranformamos el input con el word ebmedding de keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMY66HVt6RQ6"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ2aev6Q6fGW"
      },
      "source": [
        "## Codificamos train y test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JND6JK2r6ecS"
      },
      "source": [
        "train_data = tokenizer.texts_to_sequences(X_train)\n",
        "test_data = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PypjShaJ6pJj"
      },
      "source": [
        "## Rellenamos los \"huecos\" con 0 para que todas las cadenas tengan la misma longitud "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9VTTsbT6oAi"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "train_data = pad_sequences(train_data, maxlen=110, padding=\"post\") \n",
        "test_data = pad_sequences(test_data, maxlen=110, padding=\"post\") \n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb6uXIgK6vY0"
      },
      "source": [
        "\n",
        "## Creamos pesos para la clase minoritaria"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_K03byF6v7i"
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(y_train),\n",
        "                                                 y_train)\n",
        "class_weights = {k:v for k,v in enumerate(class_weights)}\n",
        "class_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN_dUqkg63az"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAS6X-5P7FqT"
      },
      "source": [
        "## Creación de la topología con una capa de convolución"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqRJTP7M65rJ"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=110, trainable=True))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', get_f1])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kONxGG007KWA"
      },
      "source": [
        "\n",
        "## Entrenamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPO5cT0N69nj"
      },
      "source": [
        "history = model.fit(train_data, y_train,\n",
        "                    epochs=15,\n",
        "                    verbose=False,\n",
        "                    validation_data=(test_data, y_test),\n",
        "                    batch_size=10,\n",
        "                    class_weight=class_weights)\n",
        "loss, accuracy, f1 = model.evaluate(train_data, y_train, verbose=False)\n",
        "print(f\"Training Accuracy: {accuracy}, Training F1: {f1}\")\n",
        "loss, accuracy, f1 = model.evaluate(test_data, y_test, verbose=False)\n",
        "print(f\"Test Accuracy: {accuracy}, Test F1: {f1}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHHPrs3k7eQo"
      },
      "source": [
        "## Resultado\n",
        "\n",
        "Podemos ver como el modelo se sobreajusta en los datos de train, pero sin embargo no es capaz de generalizar lo suficiente como para conseguir un buen f1-score en los datos de test. Para ello sería necesario aumentar el dataset. "
      ]
    }
  ]
}