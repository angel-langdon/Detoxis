{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "beto_new_strategy-stacking.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djlylIRr1nJx"
      },
      "source": [
        "### Laboratory made by:\n",
        "\n",
        "- Ignacio Cano Navarro\n",
        "- Angel Langdon Villamayor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brM1XuTR1l_y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12224086-6a9d-4d45-e586-647caa56ed47"
      },
      "source": [
        "!pip install transformers numpy torch sklearn emoji\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8MN8swU9jwb"
      },
      "source": [
        "!rm -rf logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A619k9QftLMs"
      },
      "source": [
        "# Lab 9\n",
        "\n",
        "In this lab we're going to try to further improve our two models (the one predicting toxicity and the one predicting toxicity levels) by implementing two different strategies. The first one is going to involve using the model predicting whether a comment is toxic to help the second model to classify the level of toxicity. The second one is going to be a radical change. We're going to use the first models that we tried (SVM, RF...) and put them together in a stack (we will explain this better later) to see if this strategy can improve BETO's results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s-8c-Nu6fWO"
      },
      "source": [
        "# Toxicity\n",
        "\n",
        "In this notebook, we're going to test how well BERT is able to predict toxicity and toxicity_levels variables. Lets start with toxicity "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWGLBha4sWN3"
      },
      "source": [
        "import random\n",
        "\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (AutoModel, AutoModelForSequenceClassification,\n",
        "                          AutoTokenizer, BertForSequenceClassification,\n",
        "                          BertTokenizerFast, BertTokenizer,Trainer,\n",
        "                          TrainingArguments)\n",
        "from scipy.special import softmax\n",
        "from transformers.file_utils import (is_tf_available, is_torch_available,\n",
        "                                     is_torch_tpu_available)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCa4eVxw2Bw8"
      },
      "source": [
        "Let's make a function to set a seed so we'll have same results in different runs:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgQESNWvsjnz"
      },
      "source": [
        "def set_seed(seed: int):\n",
        "    \"\"\"\n",
        "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
        "    installed).\n",
        "\n",
        "    Args:\n",
        "        seed (:obj:`int`): The seed to set.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if is_torch_available():\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # ^^ safe to call this function even if cuda is not available\n",
        "    if is_tf_available():\n",
        "        import tensorflow as tf\n",
        "\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "set_seed(1)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DC6xPBo2D_0"
      },
      "source": [
        "we'll be using bert-base-spanish-cased because we found that the cased version had a bit more f1-score than the uncased version. Also we found that the spanish bert base pretrained model worked better than the bert base version (which absolutely makes sense) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oen5PDksopU"
      },
      "source": [
        "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=False)\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy8GOjRG2i4B"
      },
      "source": [
        "Data loading and split in train/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boRJ-4Ezs4ni"
      },
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "#df_test = pd.read_csv(\"test.csv\")\n",
        "train_text = df[\"comment\"].values\n",
        "train_labels = df[\"toxicity\"].values\n",
        "train_texts, valid_texts, train_labels, valid_labels = train_test_split(list(train_text),\n",
        "                                                                     list(train_labels),\n",
        "                                                                     test_size = 0.1)\n",
        "texts = list(df[\"comment\"])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl7gb13St7rR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca480a4b-a3e9-454b-d36c-732457a09893"
      },
      "source": [
        "# max sequence length will be the average length of texts\n",
        "\n",
        "leng = [len(txt) for txt in texts]\n",
        "max_length = sum(leng)//len(leng)\n",
        "max_length\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "206"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIxVPXNn3bry"
      },
      "source": [
        "Now, lets convert our text to sequences of tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1udC7tds5JH"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
        "valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QcSkbYu3fdl"
      },
      "source": [
        "The below code wraps our tokenized text data into a torch Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izLkj4O3GbZe"
      },
      "source": [
        "class DetoxisDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXa4nblivGHY"
      },
      "source": [
        "\n",
        "# convert our tokenized data into a torch Dataset\n",
        "train_dataset = DetoxisDataset(train_encodings, train_labels)\n",
        "valid_dataset = DetoxisDataset(valid_encodings, valid_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETz9o-ku3y_B"
      },
      "source": [
        "Now that we have our data prepared, let's download and load our BERT model and its pre-trained weights. Right now we're just trying to predict whether a comment is toxic or not so the number of labels is just 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Im1l-OZvdGk",
        "outputId": "c3f820d7-6de7-4207-97ad-29415db4ba53"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(\"cuda\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1zqHgpqvgI7"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  # calculate accuracy using sklearn's function\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {'accuracy': acc,}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLmtjJyD4CL9"
      },
      "source": [
        "After loading the model, lets choose the training parameters for our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhACxFT4yKY_"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=4,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
        "    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
        "    logging_steps=200,               # log & save weights each logging_steps\n",
        "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
        "    learning_rate = 0.00001\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw0-kNAvyQXJ"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=valid_dataset,          # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "gX3EJ3woyTz0",
        "outputId": "214e750f-4ae3-4023-c421-ba8b66317280"
      },
      "source": [
        "# train the model\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [780/780 16:06, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.657000</td>\n",
              "      <td>0.630134</td>\n",
              "      <td>0.648415</td>\n",
              "      <td>8.608600</td>\n",
              "      <td>40.308000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.512200</td>\n",
              "      <td>0.558628</td>\n",
              "      <td>0.731988</td>\n",
              "      <td>8.535400</td>\n",
              "      <td>40.654000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.415500</td>\n",
              "      <td>0.577997</td>\n",
              "      <td>0.740634</td>\n",
              "      <td>8.557300</td>\n",
              "      <td>40.550000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=780, training_loss=0.4670776856251252, metrics={'train_runtime': 967.3852, 'train_samples_per_second': 0.806, 'total_flos': 1692331864908672.0, 'epoch': 4.0, 'init_mem_cpu_alloc_delta': 8192, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 14204928, 'train_mem_gpu_alloc_delta': 1780847104, 'train_mem_cpu_peaked_delta': 0, 'train_mem_gpu_peaked_delta': 2955052032})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "c2txdNDZyVuQ",
        "outputId": "91cc0a9d-f09e-4fa6-8231-ed871b1f857a"
      },
      "source": [
        "trainer.evaluate()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [18/18 00:08]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 4.0,\n",
              " 'eval_accuracy': 0.7319884726224783,\n",
              " 'eval_loss': 0.5586280226707458,\n",
              " 'eval_mem_cpu_alloc_delta': 0,\n",
              " 'eval_mem_cpu_peaked_delta': 0,\n",
              " 'eval_mem_gpu_alloc_delta': 0,\n",
              " 'eval_mem_gpu_peaked_delta': 170200064,\n",
              " 'eval_runtime': 8.6957,\n",
              " 'eval_samples_per_second': 39.905}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP1YyCWu4I-Q"
      },
      "source": [
        "After training our model, we're going to create a function that will receive a text as input and will return the model's prediction (toxic or not) as output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z5W1_-pyxJc"
      },
      "source": [
        "def get_prediction(text,max_length):\n",
        "    # prepare our text into tokenized sequence\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # perform inference to our model\n",
        "    outputs = model(**inputs)\n",
        "    # get output probabilities by doing softmax\n",
        "    probs = outputs[0].softmax(1)\n",
        "    # executing argmax function to get the candidate label\n",
        "    return probs.argmax()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POvXJysxyzl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c2fe654-5b6f-4b4e-9f95-778015f30118"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "pred = [int(get_prediction(text, max_length)) for text in valid_texts]\n",
        "score = f1_score(valid_labels, pred, average='macro')\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6904258815795652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG-QxEAo4aUk"
      },
      "source": [
        "We can see that the F1 is around 0.7 which we don't think is that much improvement, taking into account that a much simpler model such as SVM was able to have a F1 score of ~0.68 and that BERT uses lots of resources and its a lot more time consuming. Also, this is the best pretrained version of BERT that we could find, other versions such as Distile-BERT or BERT-BASE, GPT-2, BETO-Cased , twitter-roberta-base-offensive were even worse than a SVM with TFIDF matrix as input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2964jU76Ocm"
      },
      "source": [
        "## Toxicity_levels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_ayyo0U6Sxd"
      },
      "source": [
        "Now we're going to further improve the toxicity_levels model by first using our previous model that identifies whether a comment is toxic or not and then, if the model targets the comment as toxic we'll feed that comment to our new toxicity_levels predictor that will only focus on predicting the level of toxicity of a comment (excluding the possibility that the comment is not toxic, as we were doing before)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5Pc-St71EP4"
      },
      "source": [
        "def replace(df_model):\n",
        "  df_model = df_model.copy()\n",
        "  df_model['toxicity_level'] = df_model['toxicity_level'].replace(1,0)\n",
        "  df_model['toxicity_level'] = df_model['toxicity_level'].replace(2,1)\n",
        "  df_model['toxicity_level'] = df_model['toxicity_level'].replace(3,2)\n",
        "  return df_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUrFlsYr6RqT"
      },
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "#df_test = pd.read_csv(\"test.csv\")\n",
        "df_model = df.copy()\n",
        "# We are going to remove values with toxicity_level == 0\n",
        "# Get names of indexes for which column Age has value 30\n",
        "indexNames = df_model[df_model['toxicity_level'] == 0].index\n",
        "# Delete these row indexes from dataFrame\n",
        "df_model.drop(indexNames , inplace=True)\n",
        "df_model = replace(df_model)\n",
        "\n",
        "train_text = df_model[\"comment\"].values\n",
        "train_labels = df_model[\"toxicity_level\"].values\n",
        "train_texts, valid_texts, train_labels, valid_labels = train_test_split(list(train_text),\n",
        "                                                                     list(train_labels),\n",
        "                                                                     test_size = 0.2)\n",
        "texts = list(df_model[\"comment\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fU-1Y-H1qSk",
        "outputId": "293aabb6-d7a3-497e-a8a5-2435457ce6be"
      },
      "source": [
        "df_model['toxicity_level'].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_xKqple7CWm"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
        "valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "\n",
        "train_dataset = DetoxisDataset(train_encodings, train_labels)\n",
        "valid_dataset = DetoxisDataset(valid_encodings, valid_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkKGauQQ87PZ",
        "outputId": "400998ee-c6fb-45b0-e7df-182240cdde30"
      },
      "source": [
        "model_levels = BertForSequenceClassification.from_pretrained(model_name, num_labels=3).to(\"cuda\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "FJD6Z2Uc7TU3",
        "outputId": "df0849e3-46af-4092-8686-22a87b2a9d2a"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model_levels,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=valid_dataset,          # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
        ")\n",
        "\n",
        "# train the model\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='232' max='232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [232/232 04:47, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.836100</td>\n",
              "      <td>0.805472</td>\n",
              "      <td>0.665217</td>\n",
              "      <td>5.649300</td>\n",
              "      <td>40.713000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=232, training_loss=0.8153631604951004, metrics={'train_runtime': 288.6245, 'train_samples_per_second': 0.804, 'total_flos': 497492567379648.0, 'epoch': 4.0, 'init_mem_cpu_alloc_delta': 4096, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 12288, 'train_mem_gpu_alloc_delta': 1781686784, 'train_mem_cpu_peaked_delta': 0, 'train_mem_gpu_peaked_delta': 2954110976})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gupZWYWJxlrG"
      },
      "source": [
        "def get_prediction_levels(text,max_length):\n",
        "    labels = {0:1, 1:2, 2:3}\n",
        "    # prepare our text into tokenized sequence\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # perform inference to our model\n",
        "    outputs = model(**inputs)\n",
        "    # get output probabilities by doing softmax\n",
        "    probs = outputs[0].softmax(1)\n",
        "    # executing argmax function to get the candidate label\n",
        "    result = int(probs.argmax())\n",
        "    if result == 0:\n",
        "      return 0\n",
        "    else:\n",
        "      outputs = model_levels(**inputs)\n",
        "      probs = outputs[0].softmax(1)\n",
        "      result = int(probs.argmax())\n",
        "      return labels[result]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z_FWR8D7XGN",
        "outputId": "6580da98-a4f0-4fb9-f0b3-b429da93772a"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "pred = [int(get_prediction_levels(text, max_length)) for text in valid_texts]\n",
        "score = f1_score(valid_labels, pred, average='macro')\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.33712236058970646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNox6c6Q-F9G"
      },
      "source": [
        "\n",
        "We didn't expect the f1-score to be worse with this strategy than using two separate models. However there's one last technique to test in order to see if we can improve the f1-score:\n",
        "\n",
        "## Stacking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivLQ3-jadYOX"
      },
      "source": [
        "The point of stacking is to explore a space of different models for the same problem. The idea is that you can attack a learning problem with different types of models which are capable to learn some part of the problem, but not the whole space of the problem. So, you can build multiple different learners and you use them to build an intermediate prediction, one prediction for each learned model. Then you add a new model which learns from the intermediate predictions the same target.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta4yaccac8i5"
      },
      "source": [
        "As we did in previous labs we're going to implement the TF-IDF transformation. Then we'll use a stack of models such as SVM, logistic regression, decision trees, random forests... and also a metalearner that will be the best classifier of the many we have tried before (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "w6xyGBDwct3J",
        "outputId": "043b449b-bb93-4245-9b2a-9d20d3adaadc"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "\n",
        "# necessary packages\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "def delete_stop_words(comment):\n",
        "    spanish_stopwords = stopwords.words(\"spanish\")\n",
        "    return \" \".join([w for w in comment.split() if w not in spanish_stopwords])\n",
        "\n",
        "def steam(text, stemmer):\n",
        "    stemmed_text = [stemmer.stem(word) for word in word_tokenize(text)]\n",
        "    return \" \".join(stemmed_text)\n",
        "\n",
        "def clean_text_column(df, col, stemmer):\n",
        "    \"\"\"Normalizes a string column to have a processed format \n",
        "    Arguments:\n",
        "      df (pd.DataFrame): the dataframe that contains the column to normalize\n",
        "      col (str): the dataframe column to normalize\n",
        "      steammer (nltk.steam.SnowballStammer): the steammer to use for \n",
        "          steamming the text\n",
        "    Returns:\n",
        "      The dataframe with the preprocessed column\n",
        "    \"\"\"\n",
        "    df = df.copy() # copy the dataframe avoid modifying the original\n",
        "    # Make the comments to lowercase \n",
        "    df[col] = df[col].str.lower()\n",
        "    # Delete the stop words\n",
        "    df[col] = [delete_stop_words(c) for c in df[col]]\n",
        "    # Replace underscores and hyphens with spaces \n",
        "    df[col] = df[col].str.replace(\"_\", \" \")\n",
        "    df[col] = df[col].str.replace(\"-\", \" \")\n",
        "    # Create the regex to delete the urls, usernames and emojis\n",
        "    urls = r'https?://[\\S]+'\n",
        "    users = r'@[\\S]+'\n",
        "    emojis = r'[\\U00010000-\\U0010ffff]'\n",
        "    hashtags = r'\\s#[\\S]+'\n",
        "    # Join the regex\n",
        "    expr = f'''({\"|\".join([urls,\n",
        "                           users,\n",
        "                           hashtags,\n",
        "                           emojis])})'''\n",
        "    # Replace the urls, users and emojis with empty string\n",
        "    df[col] = df[col].str.replace(expr, \"\", regex=True)                      \n",
        "    # Get only the words of the text\n",
        "    df[col] = df[col].str.findall(\"\\w+\").str.join(\" \")\n",
        "    # Delete the numbers\n",
        "    df[col] = df[col].str.replace(\"[0-9]+\", \"\",regex=True)\n",
        "    # Steam the words of the text for each text in the specified column\n",
        "    #df[col] = [steam(c, stemmer) for c in  df[col]]\n",
        "    return df\n",
        "\n",
        "\n",
        "# Initialize the steammer to Spanish language\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "# read the data\n",
        "df_original = pd.read_csv(\"train.csv\") \n",
        "# Normalize the \"comment\" column\n",
        "df = clean_text_column(df_original, \"comment\", stemmer)\n",
        "df.head()\n",
        "\n",
        "\n",
        "# Create a TF-IDF with ngrams \n",
        "tfidf = TfidfVectorizer(ngram_range=(1,1))\n",
        "# Fit with the comments \n",
        "features = tfidf.fit_transform(df[\"comment\"])\n",
        "# Get the feature extraction matrix\n",
        "df_features = pd.DataFrame(features.todense(),\n",
        "             columns= tfidf.get_feature_names())\n",
        "# Print the first comment\n",
        "print(df_original[\"comment\"].iloc[0])\n",
        "# Print the sorted by probability first row of the matrix\n",
        "df_features.sort_values(by=0, axis=1, ascending=False).head(1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Pensó: Zumo para restar.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pensó</th>\n",
              "      <th>restar</th>\n",
              "      <th>zumo</th>\n",
              "      <th>opino</th>\n",
              "      <th>operativas</th>\n",
              "      <th>opinadores</th>\n",
              "      <th>opinamos</th>\n",
              "      <th>opinan</th>\n",
              "      <th>opinando</th>\n",
              "      <th>opinar</th>\n",
              "      <th>opine</th>\n",
              "      <th>opines</th>\n",
              "      <th>opinion</th>\n",
              "      <th>opiniones</th>\n",
              "      <th>opinión</th>\n",
              "      <th>opniones</th>\n",
              "      <th>operación</th>\n",
              "      <th>oponen</th>\n",
              "      <th>oportunas</th>\n",
              "      <th>oportunidad</th>\n",
              "      <th>oportunidades</th>\n",
              "      <th>oposicion</th>\n",
              "      <th>oposición</th>\n",
              "      <th>opresor</th>\n",
              "      <th>opresores</th>\n",
              "      <th>oprimidas</th>\n",
              "      <th>oprimido</th>\n",
              "      <th>oprimidos</th>\n",
              "      <th>operandi</th>\n",
              "      <th>operaciones</th>\n",
              "      <th>optas</th>\n",
              "      <th>onegetas</th>\n",
              "      <th>olía</th>\n",
              "      <th>olímpicamente</th>\n",
              "      <th>omiso</th>\n",
              "      <th>omite</th>\n",
              "      <th>omiten</th>\n",
              "      <th>omites</th>\n",
              "      <th>omitir</th>\n",
              "      <th>omnipresente</th>\n",
              "      <th>...</th>\n",
              "      <th>elijo</th>\n",
              "      <th>emigraban</th>\n",
              "      <th>emigrado</th>\n",
              "      <th>emigramos</th>\n",
              "      <th>emigran</th>\n",
              "      <th>emigrante</th>\n",
              "      <th>emigrantes</th>\n",
              "      <th>emigrar</th>\n",
              "      <th>emigraran</th>\n",
              "      <th>emigraron</th>\n",
              "      <th>emigré</th>\n",
              "      <th>emigró</th>\n",
              "      <th>embarcación</th>\n",
              "      <th>embarcaciones</th>\n",
              "      <th>embajadss</th>\n",
              "      <th>embajadas</th>\n",
              "      <th>elimina</th>\n",
              "      <th>eliminación</th>\n",
              "      <th>eliminar</th>\n",
              "      <th>eliminas</th>\n",
              "      <th>elimine</th>\n",
              "      <th>elisa</th>\n",
              "      <th>elite</th>\n",
              "      <th>eljueves</th>\n",
              "      <th>ella</th>\n",
              "      <th>ellas</th>\n",
              "      <th>elle</th>\n",
              "      <th>ello</th>\n",
              "      <th>ellos</th>\n",
              "      <th>elmundotoday</th>\n",
              "      <th>elpais</th>\n",
              "      <th>elplural</th>\n",
              "      <th>elsaltodiario</th>\n",
              "      <th>elíptica</th>\n",
              "      <th>em</th>\n",
              "      <th>ema</th>\n",
              "      <th>email</th>\n",
              "      <th>emanan</th>\n",
              "      <th>emanuel</th>\n",
              "      <th>útiles</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.630681</td>\n",
              "      <td>0.608146</td>\n",
              "      <td>0.482079</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 14289 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      pensó    restar      zumo  opino  ...  email  emanan  emanuel  útiles\n",
              "0  0.630681  0.608146  0.482079    0.0  ...    0.0     0.0      0.0     0.0\n",
              "\n",
              "[1 rows x 14289 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnRcQNcOepwO"
      },
      "source": [
        "X,y = df_features, df_original['toxicity']\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.2, random_state=20)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n873Oej-db25"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "from sklearn import svm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from mlxtend.regressor import StackingCVRegressor\n",
        "from sklearn.ensemble import StackingClassifier"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rfBekttdeyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c901d75-85fc-4b51-c335-24c25cb24621"
      },
      "source": [
        "svc = svm.SVC(class_weight='balanced')\n",
        "svc_meta = svm.SVC(class_weight='balanced')\n",
        "dt = DecisionTreeClassifier(class_weight='balanced')\n",
        "rf = RandomForestClassifier(class_weight='balanced')\n",
        "lr = LogisticRegression(class_weight='balanced')\n",
        "estimators = [('svc', svc),\n",
        "               (\"dt\", dt),\n",
        "               (\"rf\", rf),\n",
        "               (\"lr\", lr)]\n",
        "\n",
        "clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=svc_meta,\n",
        "    n_jobs=-1\n",
        ")\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StackingClassifier(cv=None,\n",
              "                   estimators=[('svc',\n",
              "                                SVC(C=1.0, break_ties=False, cache_size=200,\n",
              "                                    class_weight='balanced', coef0=0.0,\n",
              "                                    decision_function_shape='ovr', degree=3,\n",
              "                                    gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                                    probability=False, random_state=None,\n",
              "                                    shrinking=True, tol=0.001, verbose=False)),\n",
              "                               ('dt',\n",
              "                                DecisionTreeClassifier(ccp_alpha=0.0,\n",
              "                                                       class_weight='balanced',\n",
              "                                                       criter...\n",
              "                                                   solver='lbfgs', tol=0.0001,\n",
              "                                                   verbose=0,\n",
              "                                                   warm_start=False))],\n",
              "                   final_estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
              "                                       class_weight='balanced', coef0=0.0,\n",
              "                                       decision_function_shape='ovr', degree=3,\n",
              "                                       gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                                       probability=False, random_state=None,\n",
              "                                       shrinking=True, tol=0.001,\n",
              "                                       verbose=False),\n",
              "                   n_jobs=-1, passthrough=False, stack_method='auto',\n",
              "                   verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVRKFIxBgcHQ",
        "outputId": "d2601428-4237-4bc8-d2df-ad007be8f117"
      },
      "source": [
        "pred = clf.predict(X_eval)\n",
        "score = f1_score(y_eval, pred, average='macro')\n",
        "print(score)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.672077922077922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qViNQ9_Fpu7z",
        "outputId": "8c9de29e-a819-4ff6-a15b-f80aba308c36"
      },
      "source": [
        "X,y = df_features, df_original['toxicity_level']\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.2, random_state=20)\n",
        "\n",
        "svc = svm.SVC(class_weight='balanced')\n",
        "svc_meta = svm.SVC(class_weight='balanced')\n",
        "dt = DecisionTreeClassifier(class_weight='balanced')\n",
        "rf = RandomForestClassifier(class_weight='balanced')\n",
        "lr = LogisticRegression(class_weight='balanced')\n",
        "estimators = [('svc', svc),\n",
        "               (\"dt\", dt),\n",
        "               (\"rf\", rf),\n",
        "               (\"lr\", lr)]\n",
        "\n",
        "clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=svc_meta,\n",
        "    n_jobs=-1\n",
        ")\n",
        "clf.fit(X_train, y_train)\n",
        "pred = clf.predict(X_eval)\n",
        "score = f1_score(y_eval, pred, average='macro')\n",
        "print(score)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3204497857988262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYu1oaGju34e"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "After implementing the two different strategies mentioned before, we can see that it was a wasted effort because none of them we're able to improve the results seen in the last lab, that is, the results obtained with BETO. It would have been great to use gridSearchCV with the stacking strategy but training took aproximately one hour and a half without using gridsearch so we couldnt spend hours and hours of computing. After testing several strategies and having seen the results, we absolutely think that with BETO we are going to get the best f1-score and that's why we chose its prediction"
      ]
    }
  ]
}