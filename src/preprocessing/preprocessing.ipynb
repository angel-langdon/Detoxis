{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.6 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "0a7d48cc27ec634226c4ab59106716a647035b4836dc1d1b45898c175158dd5d"
        }
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.8.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfK9EUhKyUbU"
      },
      "source": [
        "# Práctica 6: Preprocesamiento y extracción de características\n",
        "### Alumnos:\n",
        "- Angel Langdon Villamayor\n",
        "- Ignacio Cano Navarro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGH6WwoHSI6P"
      },
      "source": [
        "## Librerias utilizadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBfc3wiSeM4u",
        "outputId": "7d41f80c-63e6-41ce-87c3-aa6d44cf0b34"
      },
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "\n",
        "# necessary packages\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /Users/angel/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /Users/angel/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSh040rb4s9Q"
      },
      "source": [
        "## Preprocesamiento\n",
        "En el preprocesamiento de texto se ha hecho lo siguiente:\n",
        "\n",
        "- Convertido todo el texto a minúsculas.\n",
        "- Se han eliminado las stopwords.\n",
        "- Se han sustituido guiones y barras bajas por espacios.\n",
        "- Se han eliminado urls, usuarios y emojis.\n",
        "- Los hashtags se han dejado porque hemos pensado que pueden aportar información relevante. Pero posteriormente, en el caso de que empeoren las predicciones serán eliminados.\n",
        "- Se han seleccionado solo las palabras, de esta forma se eliminan signos de puntuación y cualquier otro caracter extraño.\n",
        "- Por último, se ha realizado un stemming de las palabras para bajarlas todas al mismo nivel.\n",
        "\n",
        "NOTA: Al realizar el stemming, las palabras se han tokenizado, por ello, dobles espacios, retornos de carro, etc... han sido eliminados.\n",
        "\n",
        "Todo este preprocesamiento se ha realizado ya que llevando todo el texto al mismo nivel, reduciendo el corpus dejando solamente las palabras más importantes y también cogiendo sus \"raíces\" hace que el diccionario de palabras se reduzca drásticamente y esto ayuda a que los clasificadores que utilicemos posteriormente sean mejores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "B_W4EuRMe_WF",
        "outputId": "b31682c2-ffec-4778-e728-450a22ccb2e3"
      },
      "source": [
        "# Preprocessing\n",
        "def delete_stop_words(comment):\n",
        "    spanish_stopwords = stopwords.words(\"spanish\")\n",
        "    return \" \".join([w for w in comment.split() if w not in spanish_stopwords])\n",
        "\n",
        "def steam(text, stemmer):\n",
        "    stemmed_text = [stemmer.stem(word) for word in word_tokenize(text)]\n",
        "    return \" \".join(stemmed_text)\n",
        "\n",
        "def clean_text_column(df, col, stemmer):\n",
        "    \"\"\"Normalizes a string column to have a processed format \n",
        "    Arguments:\n",
        "      df (pd.DataFrame): the dataframe that contains the column to normalize\n",
        "      col (str): the dataframe column to normalize\n",
        "      steammer (nltk.steam.SnowballStammer): the steammer to use for \n",
        "          steamming the text\n",
        "    Returns:\n",
        "      The dataframe with the preprocessed column\n",
        "    \"\"\"\n",
        "    df = df.copy() # copy the dataframe avoid modifying the original\n",
        "    # Make the comments to lowercase \n",
        "    df[col] = df[col].str.lower()\n",
        "    # Delete the stop words\n",
        "    df[col] = [delete_stop_words(c) for c in df[col]]\n",
        "    # Replace underscores and hyphens with spaces \n",
        "    df[col] = df[col].str.replace(\"_\", \" \")\n",
        "    df[col] = df[col].str.replace(\"-\", \" \")\n",
        "    # Create the regex to delete the urls, usernames and emojis\n",
        "    urls = r'https?://[\\S]+'\n",
        "    users = r'@[\\S]+'\n",
        "    emojis = r'[\\U00010000-\\U0010ffff]'\n",
        "    # Join the regex\n",
        "    expr = f'''({\"|\".join([urls,\n",
        "                           users,\n",
        "                           emojis])})'''\n",
        "    # Replace the urls, users and emojis with empty string\n",
        "    df[col] = df[col].str.replace(expr, \"\", regex=True)                      \n",
        "    # Get only the words of the text\n",
        "    df[col] = df[col].str.findall(\"\\w+\").str.join(\" \")\n",
        "    # Delete the numbers\n",
        "    df[col] = df[col].str.replace(\"[0-9]+\", \"\",regex=True)\n",
        "    # Steam the words of the text for each text in the specified column\n",
        "    df[col] = [steam(c, stemmer) for c in  df[col]]\n",
        "    return df\n",
        "\n",
        "\n",
        "# Initialize the steammer to Spanish language\n",
        "steammer = SnowballStemmer('spanish')\n",
        "# read the data\n",
        "df_original = pd.read_csv(\"train.csv\") \n",
        "# Normalize the \"comment\" column\n",
        "df = clean_text_column(df_original, \"comment\", steammer)\n",
        "df.head()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  topic thread_id comment_id reply_to  comment_level  \\\n",
              "0    CR     0_000      0_002    0_002              1   \n",
              "1    CR     0_001      0_003    0_003              1   \n",
              "2    CR     0_002      0_004    0_004              1   \n",
              "3    CR     0_003      0_005    0_005              1   \n",
              "4    CR     0_004      0_006    0_006              1   \n",
              "\n",
              "                                             comment  argumentation  \\\n",
              "0                                      pens zum rest              0   \n",
              "1                                gust afeit sec gent              0   \n",
              "2      asi gust mat alta mar mas inmigr asi porfavor              0   \n",
              "3  loss mas valient mejor cort cabez vosotr socia...              0   \n",
              "4                                           costumbr              0   \n",
              "\n",
              "   constructiveness  positive_stance  negative_stance  ...  target_group  \\\n",
              "0                 0                0                0  ...             0   \n",
              "1                 0                0                0  ...             1   \n",
              "2                 0                0                0  ...             1   \n",
              "3                 0                0                0  ...             1   \n",
              "4                 0                0                0  ...             1   \n",
              "\n",
              "   stereotype  sarcasm  mockery  insult  improper_language  aggressiveness  \\\n",
              "0           0        0        1       0                  0               0   \n",
              "1           1        1        1       0                  0               0   \n",
              "2           0        0        0       0                  0               1   \n",
              "3           0        1        1       0                  0               0   \n",
              "4           1        0        0       0                  0               0   \n",
              "\n",
              "   intolerance  toxicity  toxicity_level  \n",
              "0            0         1               1  \n",
              "1            0         1               1  \n",
              "2            1         1               2  \n",
              "3            0         1               1  \n",
              "4            0         1               1  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic</th>\n      <th>thread_id</th>\n      <th>comment_id</th>\n      <th>reply_to</th>\n      <th>comment_level</th>\n      <th>comment</th>\n      <th>argumentation</th>\n      <th>constructiveness</th>\n      <th>positive_stance</th>\n      <th>negative_stance</th>\n      <th>...</th>\n      <th>target_group</th>\n      <th>stereotype</th>\n      <th>sarcasm</th>\n      <th>mockery</th>\n      <th>insult</th>\n      <th>improper_language</th>\n      <th>aggressiveness</th>\n      <th>intolerance</th>\n      <th>toxicity</th>\n      <th>toxicity_level</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CR</td>\n      <td>0_000</td>\n      <td>0_002</td>\n      <td>0_002</td>\n      <td>1</td>\n      <td>pens zum rest</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CR</td>\n      <td>0_001</td>\n      <td>0_003</td>\n      <td>0_003</td>\n      <td>1</td>\n      <td>gust afeit sec gent</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CR</td>\n      <td>0_002</td>\n      <td>0_004</td>\n      <td>0_004</td>\n      <td>1</td>\n      <td>asi gust mat alta mar mas inmigr asi porfavor</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CR</td>\n      <td>0_003</td>\n      <td>0_005</td>\n      <td>0_005</td>\n      <td>1</td>\n      <td>loss mas valient mejor cort cabez vosotr socia...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CR</td>\n      <td>0_004</td>\n      <td>0_006</td>\n      <td>0_006</td>\n      <td>1</td>\n      <td>costumbr</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ9PxVET1g9-"
      },
      "source": [
        "Comprobamos si el preprocesamiento ha funcionado tal y como se había mencionado en el primer apartado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aYbM9c7BJXt",
        "outputId": "16d27871-06e5-42aa-e4d6-1ab60a196311"
      },
      "source": [
        "for i, (comment, comment_original) in enumerate(zip(df[\"comment\"], df_original[\"comment\"])):\n",
        "  print(comment_original)\n",
        "  print(comment)\n",
        "  print()\n",
        "  if i == 10:\n",
        "    break\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pensó: Zumo para restar.\npens zum rest\n\nComo les gusta el afeitado en seco a esta gente.\ngust afeit sec gent\n\nasi me gusta, que se maten entre ellos y en alta mar. Mas inmigrantes asi porfavor\nasi gust mat alta mar mas inmigr asi porfavor\n\nLoss mas valientes, los que mejor cortan nuestras cabezas, Para vosotros, socialistas, izquierdistas, y no racistas, \nloss mas valient mejor cort cabez vosotr social izquierd racist\n\nCostumbres...\ncostumbr\n\nlastima que no se volvio loco del todo y se suicido de paso..\nlastim volvi loc suic pas\n\nlos mejores mas preparados para cortar calabacines, este trabaja nada mas tocar tierra.\nmejor mas prepar cort calabacin trabaj mas toc tierr\n\nLos más preparados\nprepar\n\nhombre preparado viene para currar en un matadero, o dejarle de cocinero jefe en la sede del psoe\nhombr prepar vien curr matader dej cociner jef sed pso\n\nLos detuvieron en ronda malaga, un saludo\ndetuv rond malag salud\n\npiensan que deben sacrificar un alma a cambio de sobrevivir. El otro día una mujer lanzó al mar a su bebé de pocos meses, como sacrificio\npiens deb sacrific alma cambi sobreviv dia muj lanz mar beb poc mes sacrifici\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijjv8ix54k2g"
      },
      "source": [
        "## Extracción de características\n",
        "Los algorítmos de machine learning aprenden de un conjunto de características de los datos de train. Pero el problema es que los algoritmos de machine learning no pueden trabajar con el texto directamente. Por ello, necesitamos utilizar técnicas de extración de características para convertir el texto en matrices o vectores de características.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-94kIktGH5Z"
      },
      "source": [
        "## Bag of words\n",
        "Es una forma fácil y efectiva de transformar texto en una representación numérica.\n",
        "\n",
        "En este esquema se tokeniza cada texto del corpus.\n",
        "Después se obtiene un vocabulario a partir de todos los tokens (palabras en minúsculas) encontrados en el corpus completo.\n",
        "Cada texto se representa con un vector, donde cada entrada representa las veces que aparece una palabra del vocabulario en el texto (en su versión más siemple un  valor binario, 1 si aparece menos una vez, 0 en caso contrario).\n",
        "\n",
        "A tener en cuenta:\n",
        "- El corpus se representa mediante una matriz con una fila por texto y una columna por elemento del vocabulario.\n",
        "- Es una representación del alta dimensionalidad (cada elemento del vocabulario es una característica) pero dispersa.\n",
        "- El orden de las palabras en el texto se pierde.\n",
        "\n",
        "Bag of words es uno de los algoritmos más sencillos a la hora de realizar extración de características, también, tiene muchos incovenientes como por ejemplo, la alta dimensionalidad, por todo esto, creemos que no será la opción que elijamos definitivamente. Sin embargo, la probaremos para ver si da buenos resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epgzjynJfTB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c7fb60a-d107-4871-ea93-21c23468ff60"
      },
      "source": [
        "# Bag of words\n",
        "bag_vectorizer = CountVectorizer()\n",
        "counts = bag_vectorizer.fit_transform(df[\"comment\"])\n",
        "counts\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3463x7291 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 56112 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mos6Y7VkgKUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd87c982-661a-43d3-8019-be21e39d22b5"
      },
      "source": [
        "# Change the ngram_range and use chars instead of words\n",
        "ngram_vectorizer = CountVectorizer(analyzer=\"char_wb\",\n",
        "                                   ngram_range=(3,6))\n",
        "counts = ngram_vectorizer.fit_transform(df[\"comment\"])\n",
        "counts\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3463x52027 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 730194 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pn36CMO4jhr"
      },
      "source": [
        "Como podemos observar en los ejemplos de bag of words obtenemos matrices de alta dimensionalidad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDZtnflTjPBg"
      },
      "source": [
        "### Codificación TF-IDF\n",
        "Consiste en una medida numérica que expresa cuán relevante es una palabra para un texto en un corpus.\n",
        "\n",
        "La idea con TF-IDF es reducir el peso de los términos proporcionalmente al número de textos en los que aparecen. De esta forma, el valor de un término aumenta proporcionalmente al número de veces que aparece en el texto, y es compensado por su frecuencia en el corpus.\n",
        "\n",
        "Este método podría servirnos para nuestro caso, ya que de esta forma podemos asociar ciertas palabras que se repiten en varios documentos son asociados a niveles de toxicidad. Además, esta probabilidad que nos da el algoritmo nos sirve para saber también, como de imporante es una determinada palabra o n-grama en el texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "HrQ0yfaieoYn",
        "outputId": "03db88bc-02e0-4b3e-98d3-b222223609d8"
      },
      "source": [
        "# Create a TF-IDF with ngrams from range 1 to 2\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
        "# Fit with the comments \n",
        "features = tfidf.fit_transform(df[\"comment\"])\n",
        "# Get the feature extraction matrix\n",
        "df_features = pd.DataFrame(features.todense(),\n",
        "             columns= tfidf.get_feature_names())\n",
        "# Print the first comment\n",
        "print(df_original[\"comment\"].iloc[0])\n",
        "# Print the sorted by probability first row of the matrix\n",
        "df_features.sort_values(by=0, axis=1, ascending=False).head(1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pensó: Zumo para restar.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   pens zum  zum rest       zum      rest     pens   ab  pas acept  \\\n",
              "0  0.554283  0.554283  0.397122  0.364845  0.30777  0.0        0.0   \n",
              "\n",
              "   partid disfraz  partidari  partidari descentiv  ...  espure nivel  esqu  \\\n",
              "0             0.0        0.0                  0.0  ...           0.0   0.0   \n",
              "\n",
              "   esqu problem  esquil  esquil si  esquilm  esquilm tan  esquin  \\\n",
              "0           0.0     0.0        0.0      0.0          0.0     0.0   \n",
              "\n",
              "   esquin amanci  ñol hil  \n",
              "0            0.0      0.0  \n",
              "\n",
              "[1 rows x 56756 columns]"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pens zum</th>\n      <th>zum rest</th>\n      <th>zum</th>\n      <th>rest</th>\n      <th>pens</th>\n      <th>ab</th>\n      <th>pas acept</th>\n      <th>partid disfraz</th>\n      <th>partidari</th>\n      <th>partidari descentiv</th>\n      <th>...</th>\n      <th>espure nivel</th>\n      <th>esqu</th>\n      <th>esqu problem</th>\n      <th>esquil</th>\n      <th>esquil si</th>\n      <th>esquilm</th>\n      <th>esquilm tan</th>\n      <th>esquin</th>\n      <th>esquin amanci</th>\n      <th>ñol hil</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.554283</td>\n      <td>0.554283</td>\n      <td>0.397122</td>\n      <td>0.364845</td>\n      <td>0.30777</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 56756 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6IgQxMxAzV4"
      },
      "source": [
        "Como podemos observar en el primer ejemplo de la matriz de extracción de características.\n",
        "\n",
        "Para el texto: \n",
        "- Pensó: Zumo para restar.\n",
        "\n",
        "Obtenemos probabilidades acordes al texto y que reflejan que palabras son más imporantes en este texto "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BePeFEL5u3Ty"
      },
      "source": [
        "## Conclusión\n",
        "Una vez hemos acabado con el preprocesamiento y la extracción de caraterísticas podemos comenzar a trabajar con los modelos de machine learning.\n",
        "\n",
        "En la siguiente práctica utilizaremos las matrices que obtenemos de los algoritmos de extracción de caraterísticas y nos quedaremos con la matriz que mejor precisión nos de a la hora de detectar la  toxicidad de un comentario. "
      ]
    }
  ]
}